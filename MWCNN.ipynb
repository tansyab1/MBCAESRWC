{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "MWCNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tansyab1/MBCAESRWC/blob/main/MWCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cevpu3iZqK6E"
      },
      "source": [
        "Connect the **Google Colab** with **Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKoBrqlaPXr_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed9760fb-91c6-4d50-c4c2-6f53f3174eb4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtN00cb0n6hZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fb8fb03-8667-4ed9-ce85-53059da45dff"
      },
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.7.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "os2D3RvJplgF"
      },
      "source": [
        "We will verify that GPU is enabled for this notebook\n",
        "\n",
        "Following should print: ***CUDA is available!  Training on GPU ...***\n",
        " \n",
        "If it prints otherwise, then you need to enable GPU: \n",
        "\n",
        "From **Menu** > **Runtime** > **Change Runtime Type** > **Hardware Accelerator** > **GPU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jW9T-evMrqds",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c39f5af3-3659-473f-b156-d59d04fde54f"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# check if CUDA is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if not train_on_gpu:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "    print('CUDA is available!  Training on GPU ...')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA is available!  Training on GPU ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKQGml-R_jxP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93dba42f-cd81-413f-e960-4bc3d017f2f3"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Jan 25 07:12:00 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   60C    P8    11W /  70W |     10MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VA32GQtL_sS4"
      },
      "source": [
        "\n",
        "from torch.autograd import Variable\n",
        "from torchvision import datasets\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.distributions import Categorical\n",
        "import torchvision.transforms as transforms\n",
        "import argparse\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import scipy.io as io\n",
        "import math\n",
        "import numbers\n",
        "import pywt\n",
        "from tqdm import tqdm\n",
        "from matplotlib import pyplot\n",
        "from ipywidgets import interact\n",
        "from PIL import Image\n",
        "from scipy.stats import entropy\n",
        "from collections import OrderedDict\n",
        "from skimage.feature import greycomatrix, greycoprops\n",
        "# ! git clone https://github.com/fbcotter/pytorch_wavelets\n",
        "# ! cd ./pytorch_wavelets/\n",
        "# ! pip install ./pytorch_wavelets/\n",
        "# from pytorch_wavelets import DWTForward, DWTInverse # (or import DWT, IDWT)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syX4QvGTj6rB"
      },
      "source": [
        "## 1. Define the **Loss function** including 3 main parts:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4AroTSe_ymC"
      },
      "source": [
        "def loss_fn( I_H, I_train, I_rec, score , y_train):\n",
        "    recon_loss = F.mse_loss(I_rec, I_train)\n",
        "    energy_loss = torch.norm(I_H,2)\n",
        "    classification_loss = F.mse_loss(score, y_train)\n",
        "    return recon_loss+energy_loss+classification_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pefU0CzlbUw"
      },
      "source": [
        "## 2. Define the **model**\n",
        "    Arguments:\n",
        "        split_ratio (int, sequence): ratio of training size/ total size of dataset  \n",
        "        batch_size (int, sequence): Size of the image batch for training.\n",
        "        num_class (int, sequence): number of class in dataset\n",
        "        negative_slope (float, optional): nagative parameter in ReLU function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VsgYfMLAFYI"
      },
      "source": [
        "\n",
        "\n",
        "class VAEGT(nn.Module):\n",
        "    def __init__(self, split_ratio, batch_size=1000, num_classes=10, negative_slope=0.1):\n",
        "        super(VAEGT, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.split_ratio = split_ratio\n",
        "        self.flag1 = 1 #5\n",
        "        self.flag2 = 1  #4\n",
        "\n",
        "        self.train_size = int(self.split_ratio*self.batch_size)\n",
        "        self.test_size = batch_size - self.train_size\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self.negative_slope = negative_slope\n",
        "\n",
        "        # Encoder declaration\n",
        "        self.encoder = nn.Sequential(OrderedDict([\n",
        "            ('layer1', nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1,padding=1)),\n",
        "            ('bat1', nn.BatchNorm2d(16)),\n",
        "            ('relu1', nn.LeakyReLU(negative_slope=negative_slope, inplace=True)),\n",
        "            ('layer2', nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, stride=1,padding=1)),\n",
        "            ('bat2', nn.BatchNorm2d(16)),\n",
        "            ('relu2', nn.LeakyReLU(negative_slope=negative_slope, inplace=True)),\n",
        "            ('layer3', nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, stride=1,padding=1)),\n",
        "            ('bat3', nn.BatchNorm2d(16)),\n",
        "            ('relu3', nn.LeakyReLU(negative_slope=negative_slope, inplace=True)),\n",
        "        ]))\n",
        "\n",
        "        # Decoder declaration\n",
        "        self.decoderL = nn.Sequential(OrderedDict([\n",
        "            ('layer0', nn.Conv2d(in_channels=16, out_channels=1, kernel_size=3, stride=2)),\n",
        "            ('bat1', nn.BatchNorm2d(1)),\n",
        "            ('relu0', nn.LeakyReLU(negative_slope=negative_slope, inplace=True)),\n",
        "            ('layer1',  nn.ConvTranspose2d(in_channels=1, out_channels=16, kernel_size=4, stride=2)),\n",
        "            ('bat2', nn.BatchNorm2d(16)),\n",
        "            ('relu1', nn.LeakyReLU(negative_slope=negative_slope, inplace=True)),\n",
        "            ('layer2',  nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, stride=1,padding=1)),\n",
        "            ('bat3', nn.BatchNorm2d(16)),\n",
        "            ('relu2', nn.LeakyReLU(negative_slope=negative_slope, inplace=True)),\n",
        "            ('layer3',  nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, stride=1,padding=1)),\n",
        "            ('bat4', nn.BatchNorm2d(16)),\n",
        "            ('relu3', nn.LeakyReLU(negative_slope=negative_slope, inplace=True)),\n",
        "            ('layer4',  nn.Conv2d(in_channels=16, out_channels=1, kernel_size=3, stride=1,padding=1)),\n",
        "            ('bat5', nn.BatchNorm2d(1)),\n",
        "            ('relu4', nn.LeakyReLU(negative_slope=negative_slope, inplace=True)),\n",
        "        ]))\n",
        "\n",
        "        self.decoderH = nn.Sequential(OrderedDict([\n",
        "            ('layer0', nn.Conv2d(in_channels=16, out_channels=1, kernel_size=3, stride=2)),\n",
        "            ('bat1', nn.BatchNorm2d(1)),\n",
        "            ('relu0', nn.LeakyReLU(negative_slope=negative_slope, inplace=True)),\n",
        "            ('layer1',  nn.ConvTranspose2d(in_channels=1, out_channels=16, kernel_size=4, stride=2)),\n",
        "            ('bat2', nn.BatchNorm2d(16)),\n",
        "            ('relu1', nn.LeakyReLU(negative_slope=negative_slope, inplace=True)),\n",
        "            ('layer2',  nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, stride=1,padding=1)),\n",
        "            ('bat3', nn.BatchNorm2d(16)),\n",
        "            ('relu2', nn.LeakyReLU(negative_slope=negative_slope, inplace=True)),\n",
        "            ('layer3',  nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, stride=1,padding=1)),\n",
        "            ('bat4', nn.BatchNorm2d(16)),\n",
        "            ('relu3', nn.LeakyReLU(negative_slope=negative_slope, inplace=True)),\n",
        "            ('layer4',  nn.Conv2d(in_channels=16, out_channels=1, kernel_size=3, stride=1,padding=1)),\n",
        "            ('bat5', nn.BatchNorm2d(1)),\n",
        "            ('relu4', nn.LeakyReLU(negative_slope=negative_slope, inplace=True)),\n",
        "        ]))\n",
        "\n",
        "        self.VGG = nn.Sequential(OrderedDict([\n",
        "            ('layer01', nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3,padding=1)),\n",
        "            ('bat1', nn.BatchNorm2d(64)),\n",
        "            ('relu01', nn.LeakyReLU(negative_slope=negative_slope, inplace=True)),\n",
        "            ('layer02', nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3,padding=1)),\n",
        "            ('bat2', nn.BatchNorm2d(64)),\n",
        "            ('relu02', nn.LeakyReLU(negative_slope=negative_slope, inplace=True)),\n",
        "            ('maxpool0', nn.MaxPool2d(kernel_size=2,stride=2)),\n",
        "\n",
        "            ('layer11', nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3,padding=1)),\n",
        "            ('bat3', nn.BatchNorm2d(128)),\n",
        "            ('relu11', nn.LeakyReLU(negative_slope=negative_slope, inplace=True)),\n",
        "            ('layer12', nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3,padding=1)),\n",
        "            ('bat4', nn.BatchNorm2d(128)),\n",
        "            ('relu12', nn.LeakyReLU(negative_slope=negative_slope, inplace=True)),\n",
        "            ('maxpool1', nn.MaxPool2d(kernel_size=2,stride=2)),\n",
        "\n",
        "            ('layer21', nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3,padding=1)),\n",
        "            ('bat5', nn.BatchNorm2d(256)),\n",
        "            ('relu21', nn.LeakyReLU(negative_slope=negative_slope, inplace=True)),\n",
        "            ('layer22', nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3,padding=1)),\n",
        "            ('bat6', nn.BatchNorm2d(256)),\n",
        "            ('relu22', nn.LeakyReLU(negative_slope=negative_slope, inplace=True)),\n",
        "            ('layer23', nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3,padding=1)),\n",
        "            ('bat7', nn.BatchNorm2d(256)),\n",
        "            ('relu23', nn.LeakyReLU(negative_slope=negative_slope, inplace=True)),\n",
        "            ('maxpool2', nn.MaxPool2d(kernel_size=2,stride=2)),\n",
        "\n",
        "            ('layer31', nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3,padding=1)),\n",
        "            ('bat8', nn.BatchNorm2d(512)),\n",
        "            ('relu31', nn.LeakyReLU(negative_slope=negative_slope, inplace=True)),\n",
        "            ('layer32', nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3,padding=1)),\n",
        "            ('bat9', nn.BatchNorm2d(512)),\n",
        "            ('relu32', nn.LeakyReLU(negative_slope=negative_slope, inplace=True)),\n",
        "            ('layer33', nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3,padding=1)),\n",
        "            ('bat10', nn.BatchNorm2d(512)),\n",
        "            ('relu33', nn.LeakyReLU(negative_slope=negative_slope, inplace=True)),\n",
        "            ('maxpool3', nn.MaxPool2d(kernel_size=2,stride=2)),\n",
        "\n",
        "            ('layer41', nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3,padding=1)),\n",
        "            ('bat11', nn.BatchNorm2d(512)),\n",
        "            ('relu41', nn.LeakyReLU(negative_slope=negative_slope, inplace=True)),\n",
        "            ('layer42', nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3,padding=1)),\n",
        "            ('bat12', nn.BatchNorm2d(512)),\n",
        "            ('relu42', nn.LeakyReLU(negative_slope=negative_slope, inplace=True)),\n",
        "            ('layer43', nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3,padding=1)),\n",
        "            ('bat13', nn.BatchNorm2d(512)),\n",
        "            ('relu43', nn.LeakyReLU(negative_slope=negative_slope, inplace=True)),\n",
        "            ('maxpool4', nn.MaxPool2d(kernel_size=2,stride=2)),\n",
        "            \n",
        "\n",
        "        ]))\n",
        "\n",
        "        self.classifer = nn.Sequential(OrderedDict([\n",
        "            ('linear0', nn.Linear(in_features=3072*self.flag1, out_features=4096)),\n",
        "            ('relu0', nn.LeakyReLU(negative_slope=negative_slope, inplace=True)),\n",
        "            ('dropout0', nn.Dropout(p=0.65)),\n",
        "            ('linear1', nn.Linear(in_features=4096, out_features=4096)),\n",
        "            ('relu1', nn.LeakyReLU(negative_slope=negative_slope, inplace=True)),\n",
        "            ('dropout1', nn.Dropout(p=0.65)),\n",
        "            ('linear2', nn.Linear(in_features=4096, out_features=self.num_classes)),\n",
        "\n",
        "          \n",
        "        ]))\n",
        "\n",
        "\n",
        "        self.fusion = nn.Sequential(OrderedDict([\n",
        "            ('layer01', nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3,padding=1)),\n",
        "            ('bat1', nn.BatchNorm2d(16)),\n",
        "            ('relu01', nn.LeakyReLU(negative_slope=negative_slope, inplace=True)),\n",
        "            ('layer02', nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3,padding=1)),\n",
        "            ('bat2', nn.BatchNorm2d(16)),\n",
        "            ('relu02', nn.LeakyReLU(negative_slope=negative_slope, inplace=True)),\n",
        "            ('maxpool0', nn.MaxPool2d(kernel_size=2,stride=2)),\n",
        "\n",
        "            ('layer11', nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3,padding=1)),\n",
        "            ('bat3', nn.BatchNorm2d(32)),\n",
        "            ('relu11', nn.LeakyReLU(negative_slope=negative_slope, inplace=True)),\n",
        "            ('layer12', nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3,padding=1)),\n",
        "            ('bat4', nn.BatchNorm2d(32)),\n",
        "            ('relu12', nn.LeakyReLU(negative_slope=negative_slope, inplace=True)),\n",
        "            ('maxpool1', nn.MaxPool2d(kernel_size=2,stride=2)),\n",
        "\n",
        "            ('layer21', nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3,padding=1)),\n",
        "            ('bat5', nn.BatchNorm2d(64)),\n",
        "            ('relu21', nn.LeakyReLU(negative_slope=negative_slope, inplace=True)),\n",
        "            ('layer22', nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3,padding=1)),\n",
        "            ('bat6', nn.BatchNorm2d(64)),\n",
        "            ('relu22', nn.LeakyReLU(negative_slope=negative_slope, inplace=True)),\n",
        "            ('layer23', nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3,padding=1)),\n",
        "            ('bat7', nn.BatchNorm2d(64)),\n",
        "            ('relu23', nn.LeakyReLU(negative_slope=negative_slope, inplace=True)),\n",
        "            ('maxpool2', nn.MaxPool2d(kernel_size=2,stride=2)),\n",
        "\n",
        "            ('layer31', nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3,padding=1)),\n",
        "            ('bat8', nn.BatchNorm2d(128)),\n",
        "            ('relu31', nn.LeakyReLU(negative_slope=negative_slope, inplace=True)),\n",
        "            ('layer32', nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3,padding=1)),\n",
        "            ('bat9', nn.BatchNorm2d(128)),\n",
        "            ('relu32', nn.LeakyReLU(negative_slope=negative_slope, inplace=True)),\n",
        "            ('layer33', nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3,padding=1)),\n",
        "            ('bat10', nn.BatchNorm2d(128)),\n",
        "            ('relu33', nn.LeakyReLU(negative_slope=negative_slope, inplace=True)),\n",
        "            ('maxpool3', nn.MaxPool2d(kernel_size=2,stride=2)),\n",
        "\n",
        "            ('layer41', nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3,padding=1)),\n",
        "            ('bat11', nn.BatchNorm2d(128)),\n",
        "            ('relu41', nn.LeakyReLU(negative_slope=negative_slope, inplace=True)),\n",
        "            ('layer42', nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3,padding=1)),\n",
        "            ('bat12', nn.BatchNorm2d(128)),\n",
        "            ('relu42', nn.LeakyReLU(negative_slope=negative_slope, inplace=True)),\n",
        "            ('layer43', nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3,padding=1)),\n",
        "            ('bat13', nn.BatchNorm2d(128)),\n",
        "            ('relu43', nn.LeakyReLU(negative_slope=negative_slope, inplace=True)),\n",
        "            ('maxpool4', nn.MaxPool2d(kernel_size=2,stride=2)),\n",
        "\n",
        "        ]))\n",
        "\n",
        "        self.classifer_final = nn.Sequential(OrderedDict([\n",
        "            ('linear0', nn.Linear(in_features=3072*self.flag1+768*self.flag2, out_features=2048)),\n",
        "            ('relu0', nn.LeakyReLU(negative_slope=negative_slope, inplace=True)),\n",
        "            ('dropout0', nn.Dropout(p=0.65)),\n",
        "            ('linear1', nn.Linear(in_features=2048, out_features=2048)),\n",
        "            ('relu1', nn.LeakyReLU(negative_slope=negative_slope, inplace=True)),\n",
        "            ('dropout1', nn.Dropout(p=0.65)),\n",
        "            ('linear2', nn.Linear(in_features=2048, out_features=self.num_classes)),\n",
        "\n",
        "          \n",
        "        ]))\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def forward(self, I_train, y_train):\n",
        "        if self.training:\n",
        "\n",
        "            # Encode input\n",
        "            x = self.encoder(I_train).cuda()\n",
        "            I_L = self.decoderL(x).cuda()\n",
        "            I_H = self.decoderH(x).cuda()\n",
        "            I_rec = I_L + I_H\n",
        "            \n",
        "            x_L = self.VGG(I_L).cuda()\n",
        "            x_H = self.fusion(I_H).cuda()\n",
        "            \n",
        "            x_L = x_L.view(x_L.size(0), -1)\n",
        "            x_H = x_H.view(x_H.size(0), -1)\n",
        "            x = torch.cat([x_L, x_H], dim=1)\n",
        "            score_L= self.classifer(x_L).cuda()\n",
        "            score_H= self.classifer_final(x).cuda()\n",
        "\n",
        "            score_final= score_L+score_H\n",
        "            \n",
        "            return I_rec, I_H, score_final\n",
        "\n",
        "    # def _onehot(self, y):\n",
        "    #     y_onehot = torch.FloatTensor(y.shape[0], self.num_classes)\n",
        "    #     y_onehot.zero_()\n",
        "    #     y_onehot.scatter_(1, y.long(), 1)\n",
        "    #     return y_onehot\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                m.weight.data.normal_(0, 0.01)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngS07vlQlmkl"
      },
      "source": [
        "## 3. Define the **improve checker** to save the best checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGYdTmNJAOIJ"
      },
      "source": [
        "# Accuracy checker: mode \"min\" for loss, mode \"max\" for accuracy\n",
        "class ImproveChecker():\n",
        "\tdef __init__(self, mode='min', best_val=None):\n",
        "\t\tassert mode in ['min', 'max']\n",
        "\t\tself.mode = mode\n",
        "\t\tif best_val is not None:\n",
        "\t\t\tself.best_val = best_val\n",
        "\t\telse:\n",
        "\t\t\tif self.mode=='min':\n",
        "\t\t\t\tself.best_val = np.inf\n",
        "\t\t\telif self.mode=='max':\n",
        "\t\t\t\tself.best_val = 0.0\n",
        "\n",
        "\tdef _check(self, val):\n",
        "\t\tif self.mode=='min':\n",
        "\t\t\tif val < self.best_val:\n",
        "\t\t\t\tprint(\"[%s] Improved from %.4f to %.4f\" % (self.__class__.__name__, self.best_val, val))\n",
        "\t\t\t\tself.best_val = val\n",
        "\t\t\t\treturn True\n",
        "\t\t\telse:\n",
        "\t\t\t\tprint(\"[%s] Not improved from %.4f\" % (self.__class__.__name__, self.best_val))\n",
        "\t\t\t\treturn False\n",
        "\t\telse:\n",
        "\t\t\tif val > self.best_val:\n",
        "\t\t\t\tprint(\"[%s] Improved from %.4f to %.4f\" % (self.__class__.__name__, self.best_val, val))\n",
        "\t\t\t\tself.best_val = val\n",
        "\t\t\t\treturn True\n",
        "\t\t\telse:\n",
        "\t\t\t\tprint(\"[%s] Not improved from %.4f\" % (self.__class__.__name__, self.best_val))\n",
        "\t\t\t\treturn False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_RxRhd-lwaX"
      },
      "source": [
        "## 4. Process the **data**\n",
        "    Arguments:\n",
        "        mat_path (string, sequence): link of dataset\n",
        "        split_ratio (int, sequence): split ratio of training size/ total size.\n",
        "        width, height (int, sequence): image shape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VgGHvxl7ygw"
      },
      "source": [
        " # Process data\n",
        " class MyDataset(Dataset):\n",
        "    def __init__(self, mat_path,split_ratio,width,height):       \n",
        "      self.width = width\n",
        "      self.height = height\n",
        "      self.split_ratio=split_ratio\n",
        "      feature = io.loadmat(mat_path,squeeze_me=True)['features']\n",
        "      Label = io.loadmat(mat_path,squeeze_me=True)['Label']\n",
        "\n",
        "      self.images = torch.from_numpy(np.transpose(feature)).type(torch.float)\n",
        "      self.images = torch.reshape(self.images,[len(self.images), self.width, self.height])\n",
        "\n",
        "      self.images=  torch.unsqueeze(self.images, dim=1)\n",
        "      \n",
        "      self.target = torch.from_numpy(np.transpose(Label)).type(torch.long)\n",
        "      self.data=list(zip(self.images, self.target))\n",
        "      \n",
        "      # self.per_image_mse_loss = F.mse_loss(self.init_wt, self.rec_init_wt)\n",
        "      # print('MSE loss of wavelet transform:',self.per_image_mse_loss)\n",
        "\n",
        "      self.train_size = int(self.split_ratio*len(self.data))\n",
        "      self.test_size = len(self.data) - self.train_size\n",
        "      \n",
        "    def _generate(self):\n",
        "      self.train_dataset, self.test_dataset = torch.utils.data.random_split(self.data, [self.train_size, self.test_size])\n",
        "      return self.train_dataset, self.test_dataset\n",
        "      \n",
        "    def __getitem__(self, index):\n",
        "      \n",
        "      images=self.images[index]\n",
        "      target=self.target[index]\n",
        "      return images, target\n",
        "     \n",
        "    def __len__(self):\n",
        "      return len(self.data)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYAqpCk6l4Qd"
      },
      "source": [
        "### 4.1. Declare the link as well as arguments to config the **dataset**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryz0iV6V8Fsd"
      },
      "source": [
        "mat_path = ('/content/drive/My Drive/Colab Notebooks/Master Internship/dataset/YaleB/YaleB_96x84.mat')\n",
        "split_ratio = 0.9\n",
        "num_classes = 40\n",
        "batch_size = 100\n",
        "size= [96,84] #size of input image\n",
        "\n",
        "custom_data = MyDataset(mat_path,split_ratio=split_ratio, width = size[0], height=size[1])\n",
        "data_train, data_test = custom_data._generate()\n",
        "dataloader_train = torch.utils.data.DataLoader(dataset=data_train,\n",
        "                                           num_workers=4,\n",
        "                                           batch_size= batch_size,\n",
        "                                           pin_memory=True)\n",
        "\n",
        "dataloader_test = torch.utils.data.DataLoader(dataset=data_test,\n",
        "                                           num_workers=4,\n",
        "                                           batch_size= batch_size,\n",
        "                                           pin_memory=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMtyda9FmXsI"
      },
      "source": [
        "\n",
        "## 5. Init the **model**, **optimizer** and **improvechecker**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chxlLm4MfUO6"
      },
      "source": [
        "# Initialize VAE\n",
        "model = VAEGT(split_ratio=split_ratio, batch_size= custom_data.__len__() , num_classes=50)\n",
        "model.cuda()\n",
        "\n",
        "# Optimizers\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-1)\n",
        "\n",
        "# ImproveChecker\n",
        "improvechecker = ImproveChecker(mode='min')\n",
        "\n",
        "num_epoch = 1000 #number of training epoch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhsXmxjRYgfA",
        "outputId": "eb903304-4528-486a-bfe2-36507bc3d99a"
      },
      "source": [
        "from prettytable import PrettyTable\n",
        "\n",
        "def count_parameters(model):\n",
        "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
        "    total_params = 0\n",
        "    for name, parameter in model.named_parameters():\n",
        "        if not parameter.requires_grad: continue\n",
        "        param = parameter.numel()\n",
        "        table.add_row([name, param])\n",
        "        total_params+=param\n",
        "    print(table)\n",
        "    print(f\"Total Trainable Params: {total_params}\")\n",
        "    return total_params\n",
        "\n",
        "count_parameters(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------------------+------------+\n",
            "|            Modules             | Parameters |\n",
            "+--------------------------------+------------+\n",
            "|     encoder.layer1.weight      |    144     |\n",
            "|      encoder.layer1.bias       |     16     |\n",
            "|      encoder.bat1.weight       |     16     |\n",
            "|       encoder.bat1.bias        |     16     |\n",
            "|     encoder.layer2.weight      |    2304    |\n",
            "|      encoder.layer2.bias       |     16     |\n",
            "|      encoder.bat2.weight       |     16     |\n",
            "|       encoder.bat2.bias        |     16     |\n",
            "|     encoder.layer3.weight      |    2304    |\n",
            "|      encoder.layer3.bias       |     16     |\n",
            "|      encoder.bat3.weight       |     16     |\n",
            "|       encoder.bat3.bias        |     16     |\n",
            "|     decoderL.layer0.weight     |    144     |\n",
            "|      decoderL.layer0.bias      |     1      |\n",
            "|      decoderL.bat1.weight      |     1      |\n",
            "|       decoderL.bat1.bias       |     1      |\n",
            "|     decoderL.layer1.weight     |    256     |\n",
            "|      decoderL.layer1.bias      |     16     |\n",
            "|      decoderL.bat2.weight      |     16     |\n",
            "|       decoderL.bat2.bias       |     16     |\n",
            "|     decoderL.layer2.weight     |    2304    |\n",
            "|      decoderL.layer2.bias      |     16     |\n",
            "|      decoderL.bat3.weight      |     16     |\n",
            "|       decoderL.bat3.bias       |     16     |\n",
            "|     decoderL.layer3.weight     |    2304    |\n",
            "|      decoderL.layer3.bias      |     16     |\n",
            "|      decoderL.bat4.weight      |     16     |\n",
            "|       decoderL.bat4.bias       |     16     |\n",
            "|     decoderL.layer4.weight     |    144     |\n",
            "|      decoderL.layer4.bias      |     1      |\n",
            "|      decoderL.bat5.weight      |     1      |\n",
            "|       decoderL.bat5.bias       |     1      |\n",
            "|     decoderH.layer0.weight     |    144     |\n",
            "|      decoderH.layer0.bias      |     1      |\n",
            "|      decoderH.bat1.weight      |     1      |\n",
            "|       decoderH.bat1.bias       |     1      |\n",
            "|     decoderH.layer1.weight     |    256     |\n",
            "|      decoderH.layer1.bias      |     16     |\n",
            "|      decoderH.bat2.weight      |     16     |\n",
            "|       decoderH.bat2.bias       |     16     |\n",
            "|     decoderH.layer2.weight     |    2304    |\n",
            "|      decoderH.layer2.bias      |     16     |\n",
            "|      decoderH.bat3.weight      |     16     |\n",
            "|       decoderH.bat3.bias       |     16     |\n",
            "|     decoderH.layer3.weight     |    2304    |\n",
            "|      decoderH.layer3.bias      |     16     |\n",
            "|      decoderH.bat4.weight      |     16     |\n",
            "|       decoderH.bat4.bias       |     16     |\n",
            "|     decoderH.layer4.weight     |    144     |\n",
            "|      decoderH.layer4.bias      |     1      |\n",
            "|      decoderH.bat5.weight      |     1      |\n",
            "|       decoderH.bat5.bias       |     1      |\n",
            "|       VGG.layer01.weight       |    576     |\n",
            "|        VGG.layer01.bias        |     64     |\n",
            "|        VGG.bat1.weight         |     64     |\n",
            "|         VGG.bat1.bias          |     64     |\n",
            "|       VGG.layer02.weight       |   36864    |\n",
            "|        VGG.layer02.bias        |     64     |\n",
            "|        VGG.bat2.weight         |     64     |\n",
            "|         VGG.bat2.bias          |     64     |\n",
            "|       VGG.layer11.weight       |   73728    |\n",
            "|        VGG.layer11.bias        |    128     |\n",
            "|        VGG.bat3.weight         |    128     |\n",
            "|         VGG.bat3.bias          |    128     |\n",
            "|       VGG.layer12.weight       |   147456   |\n",
            "|        VGG.layer12.bias        |    128     |\n",
            "|        VGG.bat4.weight         |    128     |\n",
            "|         VGG.bat4.bias          |    128     |\n",
            "|       VGG.layer21.weight       |   294912   |\n",
            "|        VGG.layer21.bias        |    256     |\n",
            "|        VGG.bat5.weight         |    256     |\n",
            "|         VGG.bat5.bias          |    256     |\n",
            "|       VGG.layer22.weight       |   589824   |\n",
            "|        VGG.layer22.bias        |    256     |\n",
            "|        VGG.bat6.weight         |    256     |\n",
            "|         VGG.bat6.bias          |    256     |\n",
            "|       VGG.layer23.weight       |   589824   |\n",
            "|        VGG.layer23.bias        |    256     |\n",
            "|        VGG.bat7.weight         |    256     |\n",
            "|         VGG.bat7.bias          |    256     |\n",
            "|       VGG.layer31.weight       |  1179648   |\n",
            "|        VGG.layer31.bias        |    512     |\n",
            "|        VGG.bat8.weight         |    512     |\n",
            "|         VGG.bat8.bias          |    512     |\n",
            "|       VGG.layer32.weight       |  2359296   |\n",
            "|        VGG.layer32.bias        |    512     |\n",
            "|        VGG.bat9.weight         |    512     |\n",
            "|         VGG.bat9.bias          |    512     |\n",
            "|       VGG.layer33.weight       |  2359296   |\n",
            "|        VGG.layer33.bias        |    512     |\n",
            "|        VGG.bat10.weight        |    512     |\n",
            "|         VGG.bat10.bias         |    512     |\n",
            "|       VGG.layer41.weight       |  2359296   |\n",
            "|        VGG.layer41.bias        |    512     |\n",
            "|        VGG.bat11.weight        |    512     |\n",
            "|         VGG.bat11.bias         |    512     |\n",
            "|       VGG.layer42.weight       |  2359296   |\n",
            "|        VGG.layer42.bias        |    512     |\n",
            "|        VGG.bat12.weight        |    512     |\n",
            "|         VGG.bat12.bias         |    512     |\n",
            "|       VGG.layer43.weight       |  2359296   |\n",
            "|        VGG.layer43.bias        |    512     |\n",
            "|        VGG.bat13.weight        |    512     |\n",
            "|         VGG.bat13.bias         |    512     |\n",
            "|    classifer.linear0.weight    |  12582912  |\n",
            "|     classifer.linear0.bias     |    4096    |\n",
            "|    classifer.linear1.weight    |  16777216  |\n",
            "|     classifer.linear1.bias     |    4096    |\n",
            "|    classifer.linear2.weight    |   204800   |\n",
            "|     classifer.linear2.bias     |     50     |\n",
            "|     fusion.layer01.weight      |    144     |\n",
            "|      fusion.layer01.bias       |     16     |\n",
            "|       fusion.bat1.weight       |     16     |\n",
            "|        fusion.bat1.bias        |     16     |\n",
            "|     fusion.layer02.weight      |    2304    |\n",
            "|      fusion.layer02.bias       |     16     |\n",
            "|       fusion.bat2.weight       |     16     |\n",
            "|        fusion.bat2.bias        |     16     |\n",
            "|     fusion.layer11.weight      |    4608    |\n",
            "|      fusion.layer11.bias       |     32     |\n",
            "|       fusion.bat3.weight       |     32     |\n",
            "|        fusion.bat3.bias        |     32     |\n",
            "|     fusion.layer12.weight      |    9216    |\n",
            "|      fusion.layer12.bias       |     32     |\n",
            "|       fusion.bat4.weight       |     32     |\n",
            "|        fusion.bat4.bias        |     32     |\n",
            "|     fusion.layer21.weight      |   18432    |\n",
            "|      fusion.layer21.bias       |     64     |\n",
            "|       fusion.bat5.weight       |     64     |\n",
            "|        fusion.bat5.bias        |     64     |\n",
            "|     fusion.layer22.weight      |   36864    |\n",
            "|      fusion.layer22.bias       |     64     |\n",
            "|       fusion.bat6.weight       |     64     |\n",
            "|        fusion.bat6.bias        |     64     |\n",
            "|     fusion.layer23.weight      |   36864    |\n",
            "|      fusion.layer23.bias       |     64     |\n",
            "|       fusion.bat7.weight       |     64     |\n",
            "|        fusion.bat7.bias        |     64     |\n",
            "|     fusion.layer31.weight      |   73728    |\n",
            "|      fusion.layer31.bias       |    128     |\n",
            "|       fusion.bat8.weight       |    128     |\n",
            "|        fusion.bat8.bias        |    128     |\n",
            "|     fusion.layer32.weight      |   147456   |\n",
            "|      fusion.layer32.bias       |    128     |\n",
            "|       fusion.bat9.weight       |    128     |\n",
            "|        fusion.bat9.bias        |    128     |\n",
            "|     fusion.layer33.weight      |   147456   |\n",
            "|      fusion.layer33.bias       |    128     |\n",
            "|      fusion.bat10.weight       |    128     |\n",
            "|       fusion.bat10.bias        |    128     |\n",
            "|     fusion.layer41.weight      |   147456   |\n",
            "|      fusion.layer41.bias       |    128     |\n",
            "|      fusion.bat11.weight       |    128     |\n",
            "|       fusion.bat11.bias        |    128     |\n",
            "|     fusion.layer42.weight      |   147456   |\n",
            "|      fusion.layer42.bias       |    128     |\n",
            "|      fusion.bat12.weight       |    128     |\n",
            "|       fusion.bat12.bias        |    128     |\n",
            "|     fusion.layer43.weight      |   147456   |\n",
            "|      fusion.layer43.bias       |    128     |\n",
            "|      fusion.bat13.weight       |    128     |\n",
            "|       fusion.bat13.bias        |    128     |\n",
            "| classifer_final.linear0.weight |  7864320   |\n",
            "|  classifer_final.linear0.bias  |    2048    |\n",
            "| classifer_final.linear1.weight |  4194304   |\n",
            "|  classifer_final.linear1.bias  |    2048    |\n",
            "| classifer_final.linear2.weight |   102400   |\n",
            "|  classifer_final.linear2.bias  |     50     |\n",
            "+--------------------------------+------------+\n",
            "Total Trainable Params: 57398432\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "57398432"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrBURxvCQ30n"
      },
      "source": [
        "##  8.2. Start training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWLyITDvAge0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 936
        },
        "outputId": "dd583751-48d2-4644-cdf8-2e0faed7b6c7"
      },
      "source": [
        "# Training process\n",
        "\n",
        "model.train()\n",
        "for epoch in range(1, num_epoch):\n",
        "    correct_pred = 0\n",
        "    running_loss = 0.0\n",
        "    start = time.perf_counter()\n",
        "    # Training\n",
        "    for index, (inputs_train , labels_train) in enumerate(dataloader_train): # Dataloader for training set\n",
        "      # inputs_train = inputs_train.unsqueeze(1).cuda()\n",
        "      inputs_train = inputs_train.cuda()\n",
        "      # hh_coefs_train = hh_coefs_train.cuda()\n",
        "      labels_train = labels_train.view(-1, 1).cuda()\n",
        "      y_onehot_train = torch.FloatTensor(inputs_train.shape[0], 50).cuda()\n",
        "      y_onehot_train.zero_()\n",
        "      y_onehot_train.scatter_(1, labels_train, 1).cuda()\n",
        "\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      I_rec, I_H, score_final = model(inputs_train, y_onehot_train)\n",
        "      print(y_onehot_train[0])\n",
        "      \n",
        "\n",
        "      _, pred = torch.max(score_final, 1)\n",
        "      # print(labels_train.shape)\n",
        "      correct_pred += (torch.reshape(pred,[pred.shape[0],1]) == labels_train).sum()\n",
        "\n",
        "      loss = loss_fn( I_H, inputs_train, I_rec, score_final , y_onehot_train)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "              \n",
        "      running_loss += float(loss.item())\n",
        "    end = time.perf_counter()\n",
        "    print('epoch {}/{}\\tTrain loss: {:.4f}\\tTrain accuracy: {:.2f}%'.\n",
        "        format(epoch + 1, num_epoch, running_loss / (index + 1), correct_pred.item() / (batch_size * (index + 1)) * 100))\n",
        "    print('Time: {:.2f}s'.format(end - start))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       device='cuda:0')\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       device='cuda:0')\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       device='cuda:0')\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       device='cuda:0')\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       device='cuda:0')\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       device='cuda:0')\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       device='cuda:0')\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       device='cuda:0')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-125-2b1ffedee53c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mI_H\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mI_rec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_final\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0my_onehot_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m       \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    117\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m                    )\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4BOFVgtWTnY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}